{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is an unsupervised learning technique that consists of grouping similar data sets into groups or clusters. Spectral clustering is a clustering technique that uses spectral theory to find patterns in data.\n",
    "In spectral clustering, an affinity matrix is constructed based on the similarity between the data and then spectral compilation is applied to obtain the eigenvectors and eigenvalues of the affinity matrix.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In which cases might it be more useful to apply?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The main advantage of spectral clustering is that it is capable of identifying non-spherical clusters and complex shapes.\n",
    "* It is useful with complex or non-linear structures. It is used in Detection of communities in social networks, in image segmentation\n",
    "* In the analysis of genomic data to identify groups of genes that are expressed in a similar way. \n",
    "* In Analysis of time series data such as audio and video signals, to identify events and trends in the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the mathematical fundamentals of it?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can summarize it as follows:\n",
    "\n",
    "* Build the similarity matrix: using an appropriate similarity measure, such as Euclidean distance or similarity.\n",
    "* Build the Laplacian matrix: From the similarity matrix, the Laplacian matrix is built\n",
    "* The spectral production is applied to the Laplacian matrix to obtain the eigenvectors and eigenvalues.\n",
    "* The first k eigenvectors corresponding to the k smallest eigenvalues are selected, where k is the number of clusters to be obtained.\n",
    "* The matrix of selected eigenvectors is used to project the data onto a new feature space.\n",
    "* A clustering technique is applied, such as k-means or hierarchical algorithms.\n",
    "* Clusters are assigned labels and the results are returned."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the algorithm to compute it?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spectral clustering is a clustering technique that involves using the eigenvalues and eigenvectors of a similarity matrix to partition a dataset into clusters. The algorithm to compute spectral clustering can be summarized in the following steps:\n",
    "\n",
    "* Compute the similarity matrix: Given a dataset of n data points, a similarity matrix is constructed where each entry (i, j) corresponds to the similarity between data points i and j. The choice of similarity metric depends on the problem at hand, but commonly used measures include the Gaussian similarity function, the cosine similarity, or the Euclidean distance.\n",
    "\n",
    "* Construct the Laplacian matrix: The Laplacian matrix is a graph Laplacian that is defined as L = D - W, where D is a diagonal matrix that contains the degree of each data point (i.e., the sum of the similarity scores for that data point), and W is the similarity matrix.\n",
    "\n",
    "* Compute the eigenvectors and eigenvalues of the Laplacian matrix: The eigenvectors and eigenvalues of the Laplacian matrix are computed. These eigenvalues and eigenvectors can be used to transform the original data points into a new space where clustering is easier.\n",
    "\n",
    "* Choose the number of clusters: The number of clusters is determined by examining the eigenvalues of the Laplacian matrix. The number of significant eigenvalues corresponds to the number of clusters.\n",
    "\n",
    "* Cluster the data: Finally, k-means or another clustering algorithm is used to cluster the data in the transformed space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does it hold any relation to some of the concepts previously mentioned in class? Which, and how?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of **Spectral Clustering**, the matrix that is decomposed is the Laplacian matrix constructed from the similarity matrix, and the discovered one is used to find the eigenvectors that are used in embedding and clustering."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related with PCA?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, in the sense that both involve the decomposition of a matrix into eigenvectors and eigenvalues. In the case of PCA, the matrix that is decomposed is the covariance matrix of the data, and the decomposition is used to find the principal components that explain most of the variance in the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related with SVD?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, both involve the decomposition of a matrix into its fundamental components. In the case of SVD, any matrix can be decomposed into three matrices: a matrix of singular left vectors, a diagonal matrix of singular values, and a matrix of singular right vectors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related with t-SNE?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, both algorithms are used to embed data in a lower dimensional feature space.\n",
    "In the case of t-SNE, the data is transformed into a lower-dimensional feature space using a t-student distribution instead of a Gaussian distribution, as is the case in Spectral Clustering. t-SNE is mainly used for data visualization, as it better preserves the local structure of the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related with K-Means?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spectral Clustering and K-means are two clustering algorithms used to group data into clusters, but they differ in their approach and the results they produce.\n",
    "In the case of K-means, the algorithm starts by randomly assigning the cluster centroids, then an iterative process is performed to update the centroids and assign points to the closest cluster. The goal of the algorithm is to minimize the sum of the squared distances between each point and its corresponding centroid."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related witk Kmedoids?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of K-Medoids, the algorithm starts by selecting k random points from the data set as the initial medoids of the clusters. The algorithm then iterates to update the medoids and assign points to the closest cluster. The goal of the algorithm is to minimize the sum of the squared distances between each point and its corresponding medoid."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
